{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproduction of DECAF: Generating Fair Synthetic Data Using Causally-Aware Generative Networks\n",
    "\n",
    "In this notebook we reproduce the results from the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler \n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Adult dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\n",
    "    \"age\",\n",
    "    \"workclass\",\n",
    "    \"fnlwgt\",\n",
    "    \"education\",\n",
    "    \"education-num\",\n",
    "    \"marital-status\",\n",
    "    \"occupation\",\n",
    "    \"relationship\",\n",
    "    \"race\",\n",
    "    \"sex\",\n",
    "    \"capital-gain\",\n",
    "    \"capital-loss\",\n",
    "    \"hours-per-week\",\n",
    "    \"native-country\",\n",
    "    \"income\",\n",
    "]\n",
    "def load_adult():\n",
    "    \"\"\"Load the Adult dataset in a pandas dataframe\"\"\"\n",
    "\n",
    "    path = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "    names = [\n",
    "        \"age\",\n",
    "        \"workclass\",\n",
    "        \"fnlwgt\",\n",
    "        \"education\",\n",
    "        \"education-num\",\n",
    "        \"marital-status\",\n",
    "        \"occupation\",\n",
    "        \"relationship\",\n",
    "        \"race\",\n",
    "        \"sex\",\n",
    "        \"capital-gain\",\n",
    "        \"capital-loss\",\n",
    "        \"hours-per-week\",\n",
    "        \"native-country\",\n",
    "        \"income\",\n",
    "    ]\n",
    "    df = pd.read_csv(path, names=names, index_col=False)\n",
    "    df = df.applymap(lambda x: x.strip() if type(x) is str else x)\n",
    "\n",
    "    for col in df:\n",
    "        if df[col].dtype == \"object\":\n",
    "            df = df[df[col] != \"?\"]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age         workclass  fnlwgt  education  education-num  \\\n",
       "0   39         State-gov   77516  Bachelors             13   \n",
       "1   50  Self-emp-not-inc   83311  Bachelors             13   \n",
       "2   38           Private  215646    HS-grad              9   \n",
       "3   53           Private  234721       11th              7   \n",
       "4   28           Private  338409  Bachelors             13   \n",
       "\n",
       "       marital-status         occupation   relationship   race     sex  \\\n",
       "0       Never-married       Adm-clerical  Not-in-family  White    Male   \n",
       "1  Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
       "2            Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
       "3  Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
       "4  Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week native-country income  \n",
       "0          2174             0              40  United-States  <=50K  \n",
       "1             0             0              13  United-States  <=50K  \n",
       "2             0             0              40  United-States  <=50K  \n",
       "3             0             0              40  United-States  <=50K  \n",
       "4             0             0              40           Cuba  <=50K  "
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display examples from the dataset\n",
    "adult_dataset = load_adult()\n",
    "adult_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_test_data(df):\n",
    "    \"\"\"Get GAN training data from Adult dataset\"\"\"\n",
    "    replace = [\n",
    "        [\n",
    "            \"Private\",\n",
    "            \"Self-emp-not-inc\",\n",
    "            \"Self-emp-inc\",\n",
    "            \"Federal-gov\",\n",
    "            \"Local-gov\",\n",
    "            \"State-gov\",\n",
    "            \"Without-pay\",\n",
    "            \"Never-worked\",\n",
    "        ],\n",
    "        [\n",
    "            \"Bachelors\",\n",
    "            \"Some-college\",\n",
    "            \"11th\",\n",
    "            \"HS-grad\",\n",
    "            \"Prof-school\",\n",
    "            \"Assoc-acdm\",\n",
    "            \"Assoc-voc\",\n",
    "            \"9th\",\n",
    "            \"7th-8th\",\n",
    "            \"12th\",\n",
    "            \"Masters\",\n",
    "            \"1st-4th\",\n",
    "            \"10th\",\n",
    "            \"Doctorate\",\n",
    "            \"5th-6th\",\n",
    "            \"Preschool\",\n",
    "        ],\n",
    "        [\n",
    "            \"Married-civ-spouse\",\n",
    "            \"Divorced\",\n",
    "            \"Never-married\",\n",
    "            \"Separated\",\n",
    "            \"Widowed\",\n",
    "            \"Married-spouse-absent\",\n",
    "            \"Married-AF-spouse\",\n",
    "        ],\n",
    "        [\n",
    "            \"Tech-support\",\n",
    "            \"Craft-repair\",\n",
    "            \"Other-service\",\n",
    "            \"Sales\",\n",
    "            \"Exec-managerial\",\n",
    "            \"Prof-specialty\",\n",
    "            \"Handlers-cleaners\",\n",
    "            \"Machine-op-inspct\",\n",
    "            \"Adm-clerical\",\n",
    "            \"Farming-fishing\",\n",
    "            \"Transport-moving\",\n",
    "            \"Priv-house-serv\",\n",
    "            \"Protective-serv\",\n",
    "            \"Armed-Forces\",\n",
    "        ],\n",
    "        [\n",
    "            \"Wife\",\n",
    "            \"Own-child\",\n",
    "            \"Husband\",\n",
    "            \"Not-in-family\",\n",
    "            \"Other-relative\",\n",
    "            \"Unmarried\",\n",
    "        ],\n",
    "        [\"White\", \"Asian-Pac-Islander\", \"Amer-Indian-Eskimo\", \"Other\", \"Black\"],\n",
    "        [\"Female\", \"Male\"],\n",
    "        [\n",
    "            \"United-States\",\n",
    "            \"Cambodia\",\n",
    "            \"England\",\n",
    "            \"Puerto-Rico\",\n",
    "            \"Canada\",\n",
    "            \"Germany\",\n",
    "            \"Outlying-US(Guam-USVI-etc)\",\n",
    "            \"India\",\n",
    "            \"Japan\",\n",
    "            \"Greece\",\n",
    "            \"South\",\n",
    "            \"China\",\n",
    "            \"Cuba\",\n",
    "            \"Iran\",\n",
    "            \"Honduras\",\n",
    "            \"Philippines\",\n",
    "            \"Italy\",\n",
    "            \"Poland\",\n",
    "            \"Jamaica\",\n",
    "            \"Vietnam\",\n",
    "            \"Mexico\",\n",
    "            \"Portugal\",\n",
    "            \"Ireland\",\n",
    "            \"France\",\n",
    "            \"Dominican-Republic\",\n",
    "            \"Laos\",\n",
    "            \"Ecuador\",\n",
    "            \"Taiwan\",\n",
    "            \"Haiti\",\n",
    "            \"Columbia\",\n",
    "            \"Hungary\",\n",
    "            \"Guatemala\",\n",
    "            \"Nicaragua\",\n",
    "            \"Scotland\",\n",
    "            \"Thailand\",\n",
    "            \"Yugoslavia\",\n",
    "            \"El-Salvador\",\n",
    "            \"Trinadad&Tobago\",\n",
    "            \"Peru\",\n",
    "            \"Hong\",\n",
    "            \"Holand-Netherlands\",\n",
    "        ],\n",
    "        [\">50K\", \"<=50K\"],\n",
    "    ]\n",
    "\n",
    "    for row in replace:\n",
    "        df = df.replace(row, range(len(row)))\n",
    "\n",
    "    df = df.values\n",
    "    X = df[:, :14].astype(np.uint32)\n",
    "    X = MinMaxScaler().fit_transform(X)\n",
    "    for row in X:\n",
    "        if row[9]>0:\n",
    "            row[9] = 1\n",
    "        elif row[9]<0:\n",
    "            row[9] = 0\n",
    "    y = df[:, 14].astype(np.uint8)\n",
    "\n",
    "    return train_test_split(X, y, test_size=2000, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DAG for Adult dataset\n",
    "dag = [\n",
    "    # Edges from race\n",
    "    ['race', 'occupation'],\n",
    "    ['race', 'income'],\n",
    "    ['race', 'hours-per-week'],\n",
    "    ['race', 'education'],\n",
    "    ['race', 'marital-status'],\n",
    "\n",
    "    # Edges from age\n",
    "    ['age', 'occupation'],\n",
    "    ['age', 'hours-per-week'],\n",
    "    ['age', 'income'],\n",
    "    ['age', 'workclass'],\n",
    "    ['age', 'marital-status'],\n",
    "    ['age', 'education'],\n",
    "    ['age', 'relationship'],\n",
    "    \n",
    "    # Edges from sex\n",
    "    ['sex', 'occupation'],\n",
    "    ['sex', 'marital-status'],\n",
    "    ['sex', 'income'],\n",
    "    ['sex', 'workclass'],\n",
    "    ['sex', 'education'],\n",
    "    ['sex', 'relationship'],\n",
    "    \n",
    "    # Edges from native country\n",
    "    ['native-country', 'marital-status'],\n",
    "    ['native-country', 'hours-per-week'],\n",
    "    ['native-country', 'education'],\n",
    "    ['native-country', 'workclass'],\n",
    "    ['native-country', 'income'],\n",
    "    ['native-country', 'relationship'],\n",
    "    \n",
    "    # Edges from marital status\n",
    "    ['marital-status', 'occupation'],\n",
    "    ['marital-status', 'hours-per-week'],\n",
    "    ['marital-status', 'income'],\n",
    "    ['marital-status', 'workclass'],\n",
    "    ['marital-status', 'relationship'],\n",
    "    ['marital-status', 'education'],\n",
    "    \n",
    "    # Edges from education\n",
    "    ['education', 'occupation'],\n",
    "    ['education', 'hours-per-week'],\n",
    "    ['education', 'income'],\n",
    "    ['education', 'workclass'],\n",
    "    ['education', 'relationship'],\n",
    "    \n",
    "    # All remaining edges\n",
    "    ['occupation', 'income'],\n",
    "    ['hours-per-week', 'income'],\n",
    "    ['workclass', 'income'],\n",
    "    ['relationship', 'income'],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dag_to_idx(df, dag):\n",
    "    \"\"\"Convert columns in a DAG to the corresponding indices.\"\"\"\n",
    "\n",
    "    dag_idx = []\n",
    "    for edge in dag:\n",
    "        dag_idx.append([df.columns.get_loc(edge[0]), df.columns.get_loc(edge[1])])\n",
    "\n",
    "    return dag_idx\n",
    "\n",
    "def create_bias_dict(df, edge_map):\n",
    "    \"\"\"\n",
    "    Convert the given edge tuples to a bias dict used for generating\n",
    "    debiased synthetic data.\n",
    "    \"\"\"\n",
    "    bias_dict = {}\n",
    "    for key, val in edge_map.items():\n",
    "        bias_dict[df.columns.get_loc(key)] = [df.columns.get_loc(f) for f in val]\n",
    "    \n",
    "    return bias_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8, 6], [8, 14], [8, 12], [8, 3], [8, 5], [0, 6], [0, 12], [0, 14], [0, 1], [0, 5], [0, 3], [0, 7], [9, 6], [9, 5], [9, 14], [9, 1], [9, 3], [9, 7], [13, 5], [13, 12], [13, 3], [13, 1], [13, 14], [13, 7], [5, 6], [5, 12], [5, 14], [5, 1], [5, 7], [5, 3], [3, 6], [3, 12], [3, 14], [3, 1], [3, 7], [6, 14], [12, 14], [1, 14], [7, 14]]\n"
     ]
    }
   ],
   "source": [
    "# Convert the DAG to one that can be provided to the DECAF model\n",
    "dag_seed = dag_to_idx(adult_dataset, dag)\n",
    "print(dag_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias dict FTU: {14: [9]}\n",
      "Bias dict DP: {14: [6, 12, 5, 3, 9, 1, 7]}\n",
      "Bias dict CF: {14: [5, 9]}\n"
     ]
    }
   ],
   "source": [
    "bias_dict_ftu = create_bias_dict(adult_dataset, {'income': ['sex']})\n",
    "print('Bias dict FTU:', bias_dict_ftu)\n",
    "\n",
    "bias_dict_dp = create_bias_dict(adult_dataset, {'income': [\n",
    "    'occupation', 'hours-per-week', 'marital-status', 'education', 'sex',\n",
    "    'workclass', 'relationship']})\n",
    "print('Bias dict DP:', bias_dict_dp)\n",
    "\n",
    "bias_dict_cf = create_bias_dict(adult_dataset, {'income': [\n",
    "    'marital-status', 'sex']})\n",
    "print('Bias dict CF:', bias_dict_cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>28162.000000</td>\n",
       "      <td>28162.000000</td>\n",
       "      <td>28162.000000</td>\n",
       "      <td>28162.000000</td>\n",
       "      <td>28162.000000</td>\n",
       "      <td>28162.000000</td>\n",
       "      <td>28162.000000</td>\n",
       "      <td>28162.000000</td>\n",
       "      <td>28162.000000</td>\n",
       "      <td>28162.000000</td>\n",
       "      <td>28162.000000</td>\n",
       "      <td>28162.000000</td>\n",
       "      <td>28162.000000</td>\n",
       "      <td>28162.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.293510</td>\n",
       "      <td>0.122429</td>\n",
       "      <td>0.119557</td>\n",
       "      <td>0.224993</td>\n",
       "      <td>0.608155</td>\n",
       "      <td>0.175502</td>\n",
       "      <td>0.364334</td>\n",
       "      <td>0.478340</td>\n",
       "      <td>0.111249</td>\n",
       "      <td>0.675485</td>\n",
       "      <td>0.011056</td>\n",
       "      <td>0.020284</td>\n",
       "      <td>0.407681</td>\n",
       "      <td>0.037904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.180007</td>\n",
       "      <td>0.243230</td>\n",
       "      <td>0.071738</td>\n",
       "      <td>0.228722</td>\n",
       "      <td>0.170109</td>\n",
       "      <td>0.195299</td>\n",
       "      <td>0.229230</td>\n",
       "      <td>0.246044</td>\n",
       "      <td>0.299087</td>\n",
       "      <td>0.468202</td>\n",
       "      <td>0.074709</td>\n",
       "      <td>0.092881</td>\n",
       "      <td>0.122158</td>\n",
       "      <td>0.140956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.150685</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070643</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.273973</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111862</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.410959</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.151757</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.448980</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0             1             2             3             4   \\\n",
       "count  28162.000000  28162.000000  28162.000000  28162.000000  28162.000000   \n",
       "mean       0.293510      0.122429      0.119557      0.224993      0.608155   \n",
       "std        0.180007      0.243230      0.071738      0.228722      0.170109   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.150685      0.000000      0.070643      0.066667      0.533333   \n",
       "50%        0.273973      0.000000      0.111862      0.200000      0.600000   \n",
       "75%        0.410959      0.166667      0.151757      0.266667      0.800000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                 5             6             7             8             9   \\\n",
       "count  28162.000000  28162.000000  28162.000000  28162.000000  28162.000000   \n",
       "mean       0.175502      0.364334      0.478340      0.111249      0.675485   \n",
       "std        0.195299      0.229230      0.246044      0.299087      0.468202   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.153846      0.400000      0.000000      0.000000   \n",
       "50%        0.166667      0.307692      0.400000      0.000000      1.000000   \n",
       "75%        0.333333      0.538462      0.600000      0.000000      1.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                 10            11            12            13  \n",
       "count  28162.000000  28162.000000  28162.000000  28162.000000  \n",
       "mean       0.011056      0.020284      0.407681      0.037904  \n",
       "std        0.074709      0.092881      0.122158      0.140956  \n",
       "min        0.000000      0.000000      0.000000      0.000000  \n",
       "25%        0.000000      0.000000      0.397959      0.000000  \n",
       "50%        0.000000      0.000000      0.397959      0.000000  \n",
       "75%        0.000000      0.000000      0.448980      0.000000  \n",
       "max        1.000000      1.000000      1.000000      1.000000  "
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get training and testing data\n",
    "X_train, X_test, y_train, y_test = load_train_test_data(adult_dataset)\n",
    "\n",
    "pd.DataFrame(X_train).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an MLP\n",
    "mlp = MLPClassifier().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(mlp, x = X_test, y = y_test):\n",
    "    \"\"\"Helper function that prints evaluation metrics.\"\"\"\n",
    "\n",
    "    y_pred = mlp.predict(x[:, :14])\n",
    "\n",
    "#     print('Precision:', precision_score(y, y_pred))\n",
    "#     print('Recall:', recall_score(y, y_pred))\n",
    "#     print('AUROC:', roc_auc_score(y, y_pred))\n",
    "#     print('Accuracy:', accuracy_score(y, y_pred))\n",
    "    prec = precision_score(y, y_pred)\n",
    "    rec = recall_score(y, y_pred)\n",
    "    roc = roc_auc_score(y, y_pred)\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    \n",
    "    return prec, rec, roc, acc\n",
    "\n",
    "def cal_DP(mlp, X_test):\n",
    "    \"\"\"Calculate fairness metric DP\"\"\"\n",
    "    \n",
    "    X_test_df = pd.DataFrame(X_test, columns = names[:-1]) \n",
    "    X_test_0 = X_test_df[X_test_df[\"sex\"] <0.5]\n",
    "    X_test_1 = X_test_df[X_test_df[\"sex\"] >0.5]\n",
    "    dp = abs(np.mean(mlp.predict(X_test_0)) - np.mean(mlp.predict(X_test_1)))\n",
    "    \n",
    "    #print(\"DP is: \",dp)\n",
    "    return dp\n",
    "    \n",
    "    \n",
    "# def FTU(X_test, X_synth, y_synth):\n",
    "#     \"\"\"Calculate fairness metric FTU\"\"\"\n",
    "#     X_synth_0 = np.delete(X_synth,9,1) #Delete column of protected attribute\n",
    "#     X_test_0 = np.delete(X_test,9,1) #Delete column of protected attribute\n",
    "    \n",
    "#     mlp_0 = MLPClassifier().fit(X_synth_0, y_synth)\n",
    "#     mlp_1 = MLPClassifier().fit(X_synth, y_synth)\n",
    "    \n",
    "#     ftu = abs(mlp_0.predict_proba(X_test_0) - mlp_1.predict_proba(X_test))\n",
    "    \n",
    "#     print(\"FTU is: \", np.sum(ftu)/len(X_test))\n",
    "    \n",
    "def cal_FTU(mlp, X_test):\n",
    "    \"\"\"Calculate fairness metric DP\"\"\"\n",
    "    \n",
    "    X_test_df = pd.DataFrame(X_test, columns = names[:-1])\n",
    "    X_test_0 = X_test_df.assign(sex = 0)\n",
    "    X_test_1 = X_test_df.assign(sex = 1)\n",
    "\n",
    "    ftu = abs(np.mean(mlp.predict(X_test_0)) - np.mean(mlp.predict(X_test_1)))\n",
    "    \n",
    "    #print(\"new_FTU is: \",ftu)\n",
    "    return ftu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:  0.8726355611601513 recall:  0.9214380825565912 roc_auc:  0.757907796298376\n",
      "0.20652317171145296\n",
      "0.024499999999999966\n"
     ]
    }
   ],
   "source": [
    "# Evaluate original data\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "print(\n",
    "    \"precision: \", precision_score(y_test, y_pred),\n",
    "    \"recall: \", recall_score(y_test, y_pred),\n",
    "    \"roc_auc: \", roc_auc_score(y_test, y_pred),\n",
    ")\n",
    "\n",
    "print(cal_DP(mlp, X_test))\n",
    "print(cal_FTU(mlp, X_test))\n",
    "#FTU(X_test = X_test, X_synth = X_test, y_synth = y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DECAF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | generator     | Generator_causal | 134 K \n",
      "1 | discriminator | Discriminator    | 43.6 K\n",
      "---------------------------------------------------\n",
      "178 K     Trainable params\n",
      "225       Non-trainable params\n",
      "178 K     Total params\n",
      "0.713     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised adjacency matrix as parsed:\n",
      " Parameter containing:\n",
      "tensor([[0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18c5d773b2b545c1815d69a1ec4b89c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from models.DECAF import DECAF\n",
    "from data import DataModule\n",
    "\n",
    "train_data = np.column_stack((X_train, y_train))\n",
    "dm = DataModule(train_data)\n",
    "\n",
    "model = DECAF(\n",
    "    dm.dims[0],\n",
    "    dag_seed=dag_seed,\n",
    "    h_dim=200,\n",
    "    lr=0.5e-3,\n",
    "    batch_size=64,\n",
    "    lambda_privacy=0,\n",
    "    lambda_gp=10,\n",
    "    d_updates=10,\n",
    "    alpha=2,\n",
    "    rho=2,\n",
    "    weight_decay=1e-2,\n",
    "    grad_dag_loss=False,\n",
    "    l1_g=0,\n",
    "    l1_W=1e-4,\n",
    "    p_gen=-1,\n",
    "    use_mask=True,\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=50, logger=False)\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(biased_edges={}):\n",
    "    \"\"\"Generate synthetic data which is also optionally debiased.\"\"\"\n",
    "    X_synth = (\n",
    "        model.gen_synthetic(\n",
    "            dm.dataset.x,\n",
    "            gen_order=model.get_gen_order(),\n",
    "            biased_edges=biased_edges,\n",
    "        )\n",
    "        .detach()\n",
    "        .numpy()\n",
    "    )\n",
    "\n",
    "    return X_synth[:, :14], np.rint(X_synth[:,14]).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate DECAF-ND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test on real data\n",
      "prec : 0.8112333526346265\n",
      "rec : 0.9327563249001332\n",
      "roc : 0.6390689255022755\n",
      "dp : 0.09162694317038744\n",
      "ftu : 0.03362687309139978\n",
      "**************\n",
      "test on synth data\n",
      "prec : 0.7876905702992659\n",
      "rec : 0.93\n",
      "roc : 0.5890000000000001\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic data\n",
    "X_synth, y_synth = generate_synthetic_data()\n",
    "X_synth_train, X_synth_test, y_synth_train, y_synth_test = train_test_split(X_synth, y_synth, test_size=2000, stratify=y_synth)\n",
    "\n",
    "\n",
    "# Train downstream model and eval\n",
    "print(\"test on real data\")\n",
    "ndmlp = MLPClassifier().fit(X_synth_train, y_synth_train)\n",
    "\n",
    "prec, rec, roc, acc = np.mean([eval_model(ndmlp) for i in range(1)], axis=0)\n",
    "dp = np.mean([ cal_DP(ndmlp, X_test=X_synth) for i in range(1)])\n",
    "ftu = np.mean([ cal_FTU(ndmlp, X_test=X_synth) for i in range(1)])\n",
    "print(\"prec :\",prec) \n",
    "print(\"rec :\",rec)\n",
    "print(\"roc :\",roc) \n",
    "print(\"dp :\",dp) \n",
    "print(\"ftu :\",ftu) \n",
    "#dp = DP(ndmlp, X_test=X_synth)\n",
    "#ftu = new_FTU(ndmlp, X_test=X_synth)\n",
    "\n",
    "\n",
    "#FTU(X_test = X_test, X_synth = X_synth, y_synth = y_synth)\n",
    "print(\"**************\")\n",
    "print(\"test on synth data\")\n",
    "# the weird thing they did in original code\n",
    "#eval_model(ndmlp, X_synth_test, y_synth_test)\n",
    "prec, rec, roc, acc = np.mean([eval_model(ndmlp, X_synth_test, y_synth_test) for i in range(3)], axis=0)\n",
    "print(\"prec :\",prec) \n",
    "print(\"rec :\",rec)\n",
    "print(\"roc :\",roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Users\\anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test on real data\n",
      "prec_mean : 0.7876905702992659 prec_std : 0.0\n",
      "rec_mean : 0.9454061251664448 rec_std : 1.1102230246251565e-16\n",
      "roc_mean : 0.6413777613784031 roc_std : 0.0\n",
      "acc_mean : 0.794 acc_std : 0.0\n",
      "dp_mean : 0.08765577445982464 dp_std : 0.0\n",
      "ftu_mean : 0.03174490448121581 ftu_std : 0.0\n",
      "**************\n",
      "test on synth data\n",
      "prec : 0.9915062287655719\n",
      "rec : 0.9915062287655719\n",
      "roc : 0.963701832331504\n"
     ]
    }
   ],
   "source": [
    "y_synth = ndmlp.predict(X_synth)\n",
    "X_synth_train, X_synth_test, y_synth_train, y_synth_test = train_test_split(X_synth, y_synth, test_size=2000, stratify=y_synth)\n",
    "\n",
    "# Train downstream model and eval\n",
    "ndmlp = MLPClassifier().fit(X_synth_train, y_synth_train)\n",
    "\n",
    "print(\"test on real data\")\n",
    "p_r_r_a = [eval_model(ndmlp) for i in range(3)]\n",
    "prec_mean, rec_mean, roc_mean, acc_mean = np.mean(p_r_r_a, axis=0)\n",
    "prec_std, rec_std, roc_std, acc_std = np.std(p_r_r_a, axis=0)\n",
    "\n",
    "dp = [ cal_DP(ndmlp, X_test=X_synth) for i in range(3) ]\n",
    "dp_mean = np.mean(dp)\n",
    "dp_std = np.std(dp)\n",
    "\n",
    "ftu = [ cal_FTU(ndmlp, X_test=X_synth) for i in range(3)]\n",
    "ftu_mean = np.mean(ftu)\n",
    "ftu_std = np.std(ftu)\n",
    "\n",
    "print(\"prec_mean :\", prec, \"prec_std :\", prec_std) \n",
    "print(\"rec_mean :\", rec_mean, \"rec_std :\", rec_std) \n",
    "print(\"roc_mean :\", roc_mean, \"roc_std :\", roc_std) \n",
    "print(\"acc_mean :\", acc_mean, \"acc_std :\", acc_std) \n",
    "\n",
    "print(\"dp_mean :\", dp_mean, \"dp_std :\", dp_std)\n",
    "print(\"ftu_mean :\", ftu_mean, \"ftu_std :\", ftu_std)\n",
    "\n",
    "print(\"**************\")\n",
    "print(\"test on synth data\")\n",
    "# the weird thing they did in original code\n",
    "#eval_model(ndmlp, X_synth_test, y_synth_test)\n",
    "\n",
    "prec, rec, roc, acc = np.mean([eval_model(ndmlp, X_synth_test, y_synth_test) for i in range(3)], axis=0)\n",
    "print(\"prec :\",prec) \n",
    "print(\"rec :\",rec)\n",
    "print(\"roc :\",roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate DECAF-FTU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prec : 0.7759914255091104\n",
      "rec : 0.9640479360852197\n",
      "roc : 0.5623452531831721\n",
      "dp is:  0.035994555189879596\n",
      "ftu is:  0.014239045522335014\n",
      "prec : 0.7812160694896851\n",
      "rec : 0.9479578392621871\n",
      "roc : 0.5559291270999732\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic data\n",
    "X_synth, y_synth = generate_synthetic_data(biased_edges=bias_dict_ftu)\n",
    "X_synth_train, X_synth_test, y_synth_train, y_synth_test = train_test_split(X_synth, y_synth, test_size=2000, stratify=y_synth)\n",
    "\n",
    "# Train downstream model and eval\n",
    "ftumlp = MLPClassifier().fit(X_synth_train, y_synth_train)\n",
    "prec, rec, roc, acc = eval_model(ftumlp)\n",
    "print(\"prec :\", prec) \n",
    "print(\"rec :\", rec) \n",
    "print(\"roc :\", roc) \n",
    " \n",
    "\n",
    "ftu = cal_FTU(ftumlp, X_test=X_synth)\n",
    "dp = cal_DP(ftumlp, X_test=X_synth)\n",
    "print(\"dp is: \", dp)\n",
    "print(\"ftu is: \", ftu)\n",
    "\n",
    "# the weird thing they did in original code\n",
    "prec, rec, roc, acc = eval_model(ftumlp, X_synth_test, y_synth_test)\n",
    "print(\"prec :\", prec) \n",
    "print(\"rec :\", rec) \n",
    "print(\"roc :\", roc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prec : 0.7911111111111111\n",
      "rec : 0.948069241011984\n",
      "roc : 0.5965245803453494\n",
      "dp is:  0.08916807833168294\n",
      "ftu is:  0.044670122860592354\n",
      "prec : 0.9994413407821229\n",
      "rec : 0.9988833054159687\n",
      "roc : 0.9970493082103766\n"
     ]
    }
   ],
   "source": [
    "y_synth = ftumlp.predict(X_synth)# Generate synthetic data\n",
    "X_synth_train, X_synth_test, y_synth_train, y_synth_test = train_test_split(X_synth, y_synth, test_size=2000, stratify=y_synth)\n",
    "\n",
    "ftumlp = MLPClassifier().fit(X_synth_train, y_synth_train)\n",
    "prec, rec, roc, acc = eval_model(ftumlp)\n",
    "print(\"prec :\", prec) \n",
    "print(\"rec :\", rec) \n",
    "print(\"roc :\", roc) \n",
    " \n",
    "\n",
    "ftu = cal_FTU(ftumlp, X_test=X_synth)\n",
    "dp = cal_DP(ftumlp, X_test=X_synth)\n",
    "print(\"dp is: \", dp)\n",
    "print(\"ftu is: \", ftu)\n",
    "\n",
    "# the weird thing they did in original code\n",
    "prec, rec, roc, acc = eval_model(ftumlp, X_synth_test, y_synth_test)\n",
    "print(\"prec :\", prec) \n",
    "print(\"rec :\", rec) \n",
    "print(\"roc :\", roc) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate DECAF-DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prec : 0.750126454223571\n",
      "rec : 0.9873501997336884\n",
      "roc : 0.6413777613784031\n",
      "dp is:  0.008325419801428757\n",
      "ftu is:  0.004083516795682085\n",
      "prec : 0.7626693426994481\n",
      "rec : 0.9960681520314548\n",
      "roc : 0.4990889283364025\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic data\n",
    "X_synth, y_synth = generate_synthetic_data(biased_edges=bias_dict_dp)\n",
    "X_synth_train, X_synth_test, y_synth_train, y_synth_test = train_test_split(X_synth, y_synth, test_size=2000, stratify=y_synth)\n",
    "\n",
    "\n",
    "# Train downstream model and eval\n",
    "dpmlp = MLPClassifier().fit(X_synth_train, y_synth_train)\n",
    "prec, rec, roc, acc = eval_model(dpmlp)\n",
    "print(\"prec :\", prec) \n",
    "print(\"rec :\", rec) \n",
    "print(\"roc :\", roc_mean) \n",
    "\n",
    "ftu = cal_FTU(dpmlp, X_test=X_synth)\n",
    "dp = cal_DP(dpmlp, X_test=X_synth)\n",
    "print(\"dp is: \", dp)\n",
    "print(\"ftu is: \", ftu)\n",
    "\n",
    "\n",
    "# the weird thing they did in original code\n",
    "prec, rec, roc, acc = eval_model(dpmlp, X_synth_test, y_synth_test)\n",
    "print(\"prec :\", prec) \n",
    "print(\"rec :\", rec) \n",
    "print(\"roc :\", roc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prec : 0.7503782148260212\n",
      "rec : 0.9906790945406125\n",
      "roc : 0.49835159546307733\n",
      "dp is:  0.004342159983573901\n",
      "ftu is:  0.0012073006178539014\n",
      "prec : 0.9994987468671679\n",
      "rec : 1.0\n",
      "roc : 0.9166666666666667\n"
     ]
    }
   ],
   "source": [
    "y_synth = dpmlp.predict(X_synth)# Generate synthetic data\n",
    "X_synth_train, X_synth_test, y_synth_train, y_synth_test = train_test_split(X_synth, y_synth, test_size=2000, stratify=y_synth)\n",
    "\n",
    "dpmlp = MLPClassifier().fit(X_synth_train, y_synth_train)\n",
    "prec, rec, roc, acc = eval_model(dpmlp)\n",
    "print(\"prec :\", prec) \n",
    "print(\"rec :\", rec) \n",
    "print(\"roc :\", roc) \n",
    "\n",
    "ftu = cal_FTU(dpmlp, X_test=X_synth)\n",
    "dp = cal_DP(dpmlp, X_test=X_synth)\n",
    "print(\"dp is: \", dp)\n",
    "print(\"ftu is: \", ftu)\n",
    "\n",
    "\n",
    "# the weird thing they did in original code\n",
    "prec, rec, roc, acc = eval_model(dpmlp, X_synth_test, y_synth_test)\n",
    "print(\"prec :\", prec) \n",
    "print(\"rec :\", rec) \n",
    "print(\"roc :\", roc) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate DECAF-CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prec : 0.7656492498706674\n",
      "rec : 0.9853528628495339\n",
      "roc : 0.5378571543163332\n",
      "dp is:  0.016329974763844235\n",
      "ftu is:  0.015446346140188916\n",
      "prec : 0.7696600710299341\n",
      "rec : 0.9915032679738562\n",
      "roc : 0.5127729105826728\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic data\n",
    "X_synth, y_synth = generate_synthetic_data(biased_edges=bias_dict_cf)\n",
    "X_synth_train, X_synth_test, y_synth_train, y_synth_test = train_test_split(X_synth, y_synth, test_size=2000, stratify=y_synth)\n",
    "\n",
    "\n",
    "# Train downstream model and eval\n",
    "cfmlp = MLPClassifier().fit(X_synth_train, y_synth_train)\n",
    "eval_model(cfmlp)\n",
    "\n",
    "prec, rec, roc, acc = eval_model(cfmlp)\n",
    "print(\"prec :\", prec) \n",
    "print(\"rec :\", rec) \n",
    "print(\"roc :\", roc) \n",
    "\n",
    "ftu = cal_FTU(cfmlp, X_test=X_synth)\n",
    "dp = cal_DP(cfmlp, X_test=X_synth)\n",
    "print(\"dp is: \", dp)\n",
    "print(\"ftu is: \", ftu)\n",
    "\n",
    "# the weird thing they did in original code\n",
    "prec, rec, roc, acc = eval_model(cfmlp, X_synth_test, y_synth_test)\n",
    "print(\"prec :\", prec) \n",
    "print(\"rec :\", rec) \n",
    "print(\"roc :\", roc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prec : 0.7768331562167906\n",
      "rec : 0.9733688415446072\n",
      "roc : 0.5649976737843517\n",
      "dp is:  0.01673258101112096\n",
      "ftu is:  0.028123002627654214\n",
      "prec : 0.9957424161788185\n",
      "rec : 0.9492643328259767\n",
      "roc : 0.8367011319302297\n"
     ]
    }
   ],
   "source": [
    "y_synth = cfmlp.predict(X_synth)# Generate synthetic data\n",
    "X_synth_train, X_synth_test, y_synth_train, y_synth_test = train_test_split(X_synth, y_synth, test_size=2000, stratify=y_synth)\n",
    "\n",
    "cfmlp = MLPClassifier().fit(X_synth_train, y_synth_train)\n",
    "prec, rec, roc, acc = eval_model(dpmlp)\n",
    "print(\"prec :\", prec) \n",
    "print(\"rec :\", rec) \n",
    "print(\"roc :\", roc) \n",
    "\n",
    "ftu = cal_FTU(dpmlp, X_test=X_synth)\n",
    "dp = cal_DP(dpmlp, X_test=X_synth)\n",
    "print(\"dp is: \", dp)\n",
    "print(\"ftu is: \", ftu)\n",
    "\n",
    "\n",
    "# the weird thing they did in original code\n",
    "prec, rec, roc, acc = eval_model(dpmlp, X_synth_test, y_synth_test)\n",
    "print(\"prec :\", prec) \n",
    "print(\"rec :\", rec) \n",
    "print(\"roc :\", roc) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fairness Calcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9334600760456274\n",
      "Recall: 0.6537949400798935\n",
      "AUROC: 0.7566163455419548\n",
      "Accuracy: 0.705\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-164-124bebeb7d65>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0meval_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mftumlp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mcompute_FTU\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
