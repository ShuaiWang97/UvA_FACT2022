{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduction of Credit dataset experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we reproduce the results from Figure 3 of the DECAF paper. We compare various injected bias strength using the DECAF model. As described in the paper we run all experiments (as implemented in this notebook) 10 times and avarage the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from data import DataModule, inject_synth_bias, load_credit, preprocess_credit\n",
    "from metrics import eval_model\n",
    "from models.DECAF import DECAF\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess credit dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= load_credit()\n",
    "credit_data = preprocess_credit(df)\n",
    "names = list(credit_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the dag used in paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DAG for Credit dataset\n",
    "credit_dag= [    \n",
    "    # Edges from age\n",
    "    ['age', 'yearsemployed'],\n",
    "    \n",
    "    # Edges from ethnicity\n",
    "    ['ethnicity', 'approved'],\n",
    "    ['ethnicity', 'married'],\n",
    "    \n",
    "    # Edges from default\n",
    "    [\"priordefault\", \"creditscore\"],\n",
    "    [\"priordefault\", \"approved\"],\n",
    "    [\"priordefault\", \"employed\"],\n",
    "    \n",
    "    # Edges from zip\n",
    "    [\"zip\", \"married\"],\n",
    "    # Edges from citizen\n",
    "    [\"citizen\",\"married\"],\n",
    "    # Edges from driverslicense\n",
    "    [\"driverslicense\",\"employed\"],\n",
    "    # Edges from education_level\n",
    "    [\"educationlevel\",\"employed\"],\n",
    "    [\"educationlevel\",\"married\"],\n",
    "    \n",
    "    # Edges from yearsemployed\n",
    "    [\"yearsemployed\", \"creditscore\"],\n",
    "    # Edges from creditscore\n",
    "    [\"creditscore\", \"approved\"],\n",
    "    [\"creditscore\", \"debt\"],\n",
    "    \n",
    "    # Edges from employed\n",
    "    [\"employed\", \"bankcustomer\"],\n",
    "    [\"employed\", \"debt\"],\n",
    "    [\"employed\", \"citizen\"],\n",
    "    \n",
    "    # Edges from debt\n",
    "    [\"debt\", \"income\"],\n",
    "    # Edges from married\n",
    "    [\"married\", \"approved\"],\n",
    "    \n",
    "    # Edges from income\n",
    "    [\"income\", \"approved\"],\n",
    "    [\"income\", \"married\"],\n",
    "]\n",
    "\n",
    "def dag_to_idx(df, dag):\n",
    "    \"\"\"Convert columns in a DAG to the corresponding indices.\"\"\"\n",
    "\n",
    "    dag_idx = []\n",
    "    for edge in dag:\n",
    "        dag_idx.append([df.columns.get_loc(edge[0]), df.columns.get_loc(edge[1])])\n",
    "\n",
    "    return dag_idx\n",
    "\n",
    "#Convert the DAG to one that can be provided to the DECAF model\n",
    "dag_seed_paper = dag_to_idx(credit_data, credit_dag)\n",
    "print(\"dag_seed of paper: \",dag_seed_paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define edge for different fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bias_dict(df, edge_map):\n",
    "    \"\"\"\n",
    "    Convert the given edge tuples to a bias dict used for generating\n",
    "    debiased synthetic data.\n",
    "    \"\"\"\n",
    "    bias_dict = {}\n",
    "    for key, val in edge_map.items():\n",
    "        bias_dict[df.columns.get_loc(key)] = [df.columns.get_loc(f) for f in val]\n",
    "    \n",
    "    return bias_dict\n",
    "\n",
    "bias_dict_nd = {}\n",
    "print('Bias dict ND:', bias_dict_nd)\n",
    "\n",
    "# Bias dictionary to satisfy FTU\n",
    "bias_dict_ftu = create_bias_dict(credit_data, {'approved': ['ethnicity']})\n",
    "print('Bias dict FTU:', bias_dict_ftu)\n",
    "\n",
    "# Bias dictionary to satisfy DP\n",
    "bias_dict_dp = create_bias_dict(credit_data, {'approved': ['married','ethnicity']})\n",
    "print('Bias dict DP:', bias_dict_dp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debais_orig_credit():\n",
    "    label = \"approved\"\n",
    "    df= load_credit()\n",
    "    credit_data = preprocess_credit(df)\n",
    "    dataset_train, dataset_test = train_test_split(credit_data, test_size=0.2, random_state=42)\n",
    "\n",
    "    total_result = {'precision': [], 'recall': [], 'auroc': [], 'dp': [], 'ftu': []}\n",
    "    for i in range(10):\n",
    "        dataset_train, dataset_test = train_test_split(credit_data, test_size=0.2, random_state=42)\n",
    "        single_result = eval_model(dataset_train = dataset_train, dataset_test = dataset_test)\n",
    "        for key in total_result:\n",
    "            total_result[key].append(single_result[key])\n",
    "\n",
    "        final_result = []\n",
    "    for key in total_result:\n",
    "        final_result += [np.mean(total_result[key])]\n",
    "        final_result += [np.std(total_result[key])]\n",
    "    print(\"final_result: \",final_result)\n",
    "        \n",
    "    return final_result\n",
    "\n",
    "\n",
    "col_names =['name', 'precision_mean', 'precision_std', 'recall_mean','recall_std', 'auroc_mean', 'auroc_std','dp_mean','dp_std', 'ftu_mean','ftu_std']\n",
    "df = pd.DataFrame(columns = col_names)\n",
    "orig_result = debais_orig_credit()\n",
    "df.loc[len(df)] =([\"ftu\"]+orig_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Train DECAF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = './cache/'\n",
    "def train_decaf(train_dataset, dag_seed, biased_edges={}, dataset=\"adult\",label=\"income\", bias=0, h_dim=200, lr=0.5e-3,\n",
    "                batch_size=64, lambda_privacy=0, lambda_gp=10, d_updates=10,\n",
    "                alpha=2, rho=2, weight_decay=1e-2, grad_dag_loss=False, l1_g=0,\n",
    "                l1_W=1e-4, p_gen=-1, use_mask=True, epochs=50):\n",
    "    model_filename = os.path.join(models_dir, 'decaf_'+dataset+str(bias)+'.pkl')\n",
    "    dm = DataModule(train_dataset.values)\n",
    "\n",
    "    model = DECAF(\n",
    "        dm.dims[0],\n",
    "        dag_seed=dag_seed,\n",
    "        h_dim=h_dim,\n",
    "        lr=lr,\n",
    "        batch_size=batch_size,\n",
    "        lambda_privacy=lambda_privacy,\n",
    "        lambda_gp=lambda_gp,\n",
    "        d_updates=d_updates,\n",
    "        alpha=alpha,\n",
    "        rho=rho,\n",
    "        weight_decay=weight_decay,\n",
    "        grad_dag_loss=grad_dag_loss,\n",
    "        l1_g=l1_g,\n",
    "        l1_W=l1_W,\n",
    "        p_gen=p_gen,\n",
    "        use_mask=use_mask,\n",
    "    )\n",
    "    print(\"model name: \",model_filename)\n",
    "    if os.path.exists(model_filename):\n",
    "        model = torch.load(model_filename)\n",
    "    else:\n",
    "        trainer = pl.Trainer(max_epochs=epochs, logger=False)\n",
    "        trainer.fit(model, dm)\n",
    "        torch.save(model, model_filename)\n",
    "\n",
    "    # Generate synthetic data\n",
    "    synth_dataset = (\n",
    "        model.gen_synthetic(\n",
    "            dm.dataset.x,\n",
    "            gen_order=model.get_gen_order(),\n",
    "            biased_edges=biased_edges,\n",
    "        )\n",
    "        .detach()\n",
    "        .numpy()\n",
    "    )\n",
    "    synth_dataset[:, -1] = synth_dataset[:, -1].astype(np.int8)\n",
    "\n",
    "    synth_dataset = pd.DataFrame(synth_dataset,\n",
    "                                 index=train_dataset.index,\n",
    "                                 columns=train_dataset.columns)\n",
    "    synth_dataset[\"ethnicity\"] = np.round(synth_dataset[\"ethnicity\"])\n",
    "\n",
    "    return synth_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def injected_bias(biased_edges={},name=\"nd\"):\n",
    "    # calculate injected bias result and save to dataframe\n",
    "    col_names =['bias','name', 'precision_mean', 'precision_std', 'recall_mean','recall_std', 'auroc_mean', 'auroc_std','dp_mean','dp_std', 'ftu_mean','ftu_std']\n",
    "    df = pd.DataFrame(columns = col_names)\n",
    "    \n",
    "    label = \"approved\"\n",
    "    data= load_credit()\n",
    "    credit_data = preprocess_credit(data)\n",
    "\n",
    "    for bias in [0,0.25,0.5,0.75,1]:\n",
    "        biased_data = inject_synth_bias(credit_data, bias=bias)\n",
    "        if bias<0:\n",
    "            biased_data = credit_data\n",
    "            \n",
    "        biased_train, biased_test = train_test_split(biased_data, test_size=0.2)\n",
    "        X_train, y_train = biased_train.drop(columns=[label]), biased_train[label]\n",
    "        mlp = MLPClassifier().fit(X_train, y_train)\n",
    "        \n",
    "\n",
    "        #calculate DECAF-ND\n",
    "        total_result = {'precision': [], 'recall': [], 'auroc': [], 'dp': [], 'ftu': []}\n",
    "        for i in range(10):\n",
    "            synth_data = train_decaf(biased_data, dag_seed_paper, dataset=\"credit\",label=\"approved\",bias=bias, epochs=250,biased_edges=biased_edges)\n",
    "            y_synth = mlp.predict(synth_data.drop(columns=[label]))\n",
    "            synth_data[label]= y_synth\n",
    "            single_result = eval_model(dataset_train = synth_data, dataset_test = biased_data)\n",
    "\n",
    "            for key in total_result:\n",
    "                total_result[key].append(single_result[key])\n",
    "        \n",
    "        final_result = []\n",
    "        for key in total_result:\n",
    "            final_result += [np.mean(total_result[key])]\n",
    "            final_result += [np.std(total_result[key])]\n",
    "        print(\"final_result: \",final_result)\n",
    "        df.loc[len(df)] =([bias]+[name]+final_result)\n",
    "    \n",
    "    df.to_csv(\"credit_\"+name+\".csv\", index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def debais_credit(biased_edges={}):\n",
    "    label = \"approved\"\n",
    "    df= load_credit()\n",
    "    credit_data = preprocess_credit(df)\n",
    "    dataset_train, dataset_test = train_test_split(credit_data, test_size=0.2, random_state=42)\n",
    "    X_train, y_train = dataset_train.drop(columns=[label]), dataset_train[label]\n",
    "    mlp = MLPClassifier().fit(X_train, y_train)\n",
    "\n",
    "    total_result = {'precision': [], 'recall': [], 'auroc': [], 'dp': [], 'ftu': []}\n",
    "    for i in range(10):\n",
    "        synth_data = train_decaf(credit_data, dag_seed_paper, dataset=\"credit\",label=\"approved\",bias=-1, epochs=250,biased_edges=bias_dict_ftu)\n",
    "        y_synth_train = mlp.predict(synth_data.drop(columns=[label]))\n",
    "        synth_data[label]= y_synth_train\n",
    "        single_result = eval_model(dataset_train = dataset_train, dataset_test = dataset_test)\n",
    "        for key in total_result:\n",
    "            total_result[key].append(single_result[key])\n",
    "\n",
    "        final_result = []\n",
    "    for key in total_result:\n",
    "        final_result += [np.mean(total_result[key])]\n",
    "        final_result += [np.std(total_result[key])]\n",
    "    print(\"final_result: \",final_result)\n",
    "        \n",
    "    return final_result\n",
    "\n",
    "\n",
    "col_names =['name', 'precision_mean', 'precision_std', 'recall_mean','recall_std', 'auroc_mean', 'auroc_std','dp_mean','dp_std', 'ftu_mean','ftu_std']\n",
    "df = pd.DataFrame(columns = col_names)\n",
    "ftu_result = debais_credit(biased_edges=bias_dict_ftu)\n",
    "df.loc[len(df)] =([\"ftu\"]+ftu_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftu_result = debais_credit(biased_edges=bias_dict_ftu)\n",
    "df.loc[len(df)] =([\"ftu\"]+ftu_result)\n",
    "nd_result = debais_credit(biased_edges=bias_dict_nd)\n",
    "df.loc[len(df)] =([\"nd\"]+nd_result)\n",
    "dp_result = debais_credit(biased_edges=bias_dict_dp)\n",
    "df.loc[len(df)] =([\"dp\"]+dp_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DECAF-ND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate DECAF ND\n",
    "injected_bias(biased_edges=bias_dict_nd,name=\"decaf_nd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DECAF-FTU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#calculate DECAF FTU\n",
    "injected_bias(biased_edges=bias_dict_ftu,name=\"decaf_ftu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DECAF-DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate DECAF DP\n",
    "injected_bias(biased_edges=bias_dict_dp,name=\"decaf_DP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_ftu = pd.read_csv(\"credit_decaf_ftu.csv\")\n",
    "result_nd = pd.read_csv(\"credit_decaf_nd.csv\")\n",
    "result_dp = pd.read_csv(\"credit_decaf_dp.csv\")\n",
    "frames = [result_ftu, result_nd, result_dp]\n",
    "\n",
    "whole_result = pd.concat(frames)\n",
    "whole_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "fig, [ax1, ax2, ax3, ax4, ax5] = plt.subplots(nrows=1, ncols=5,figsize=(15,3))\n",
    " \n",
    "for key, group in whole_result.groupby('name'):\n",
    "    group.plot('bias', 'precision_mean', yerr='precision_std',\n",
    "               label=key, ax=ax1)\n",
    "ax1.title.set_text('precision')\n",
    "\n",
    "for key, group in whole_result.groupby('name'):\n",
    "    group.plot('bias', 'recall_mean', yerr='recall_std',\n",
    "               label=key, ax=ax2)\n",
    "ax2.title.set_text('recall')\n",
    "    \n",
    "for key, group in whole_result.groupby('name'):\n",
    "    group.plot('bias', 'auroc_mean', yerr='auroc_std',\n",
    "               label=key, ax=ax3)\n",
    "ax3.title.set_text('auroc')\n",
    "\n",
    "\n",
    "for key, group in whole_result.groupby('name'):\n",
    "    group.plot('bias', 'dp_mean', yerr='dp_std',\n",
    "               label=key, ax=ax4)\n",
    "ax4.title.set_text('dp')\n",
    "\n",
    "for key, group in whole_result.groupby('name'):\n",
    "    group.plot('bias', 'ftu_mean', yerr='ftu_std',\n",
    "               label=key, ax=ax5)\n",
    "ax5.title.set_text('ftu')\n",
    "fig.savefig(\"./exp2.pdf\",dpi=150)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
